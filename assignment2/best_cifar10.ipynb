{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_val:  (1000, 3, 32, 32)\n",
      "X_train:  (49000, 3, 32, 32)\n",
      "X_test:  (1000, 3, 32, 32)\n",
      "y_val:  (1000,)\n",
      "y_train:  (49000,)\n",
      "y_test:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# load cifar10 data\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.iteritems():\n",
    "  print '%s: ' % k, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 490) loss: 2.300330\n",
      "(Epoch 0 / 1) train acc: 0.093000; val_acc: 0.098000\n",
      "(Iteration 2 / 490) loss: 2.320084\n",
      "(Iteration 3 / 490) loss: 2.297181\n",
      "(Iteration 4 / 490) loss: 2.234770\n",
      "(Iteration 5 / 490) loss: 2.209150\n",
      "(Iteration 6 / 490) loss: 2.130695\n",
      "(Iteration 7 / 490) loss: 2.071850\n",
      "(Iteration 8 / 490) loss: 2.105047\n",
      "(Iteration 9 / 490) loss: 2.183353\n",
      "(Iteration 10 / 490) loss: 1.982717\n",
      "(Iteration 11 / 490) loss: 2.040684\n",
      "(Iteration 12 / 490) loss: 1.962295\n",
      "(Iteration 13 / 490) loss: 2.013854\n",
      "(Iteration 14 / 490) loss: 1.914338\n",
      "(Iteration 15 / 490) loss: 2.047537\n",
      "(Iteration 16 / 490) loss: 1.887263\n",
      "(Iteration 17 / 490) loss: 1.962918\n",
      "(Iteration 18 / 490) loss: 1.752961\n",
      "(Iteration 19 / 490) loss: 1.868144\n",
      "(Iteration 20 / 490) loss: 1.803471\n",
      "(Iteration 21 / 490) loss: 1.965546\n",
      "(Iteration 22 / 490) loss: 1.946427\n",
      "(Iteration 23 / 490) loss: 1.840394\n",
      "(Iteration 24 / 490) loss: 1.833292\n",
      "(Iteration 25 / 490) loss: 1.678495\n",
      "(Iteration 26 / 490) loss: 1.915433\n",
      "(Iteration 27 / 490) loss: 1.561155\n",
      "(Iteration 28 / 490) loss: 1.810407\n",
      "(Iteration 29 / 490) loss: 1.594336\n",
      "(Iteration 30 / 490) loss: 1.589156\n",
      "(Iteration 31 / 490) loss: 1.760594\n",
      "(Iteration 32 / 490) loss: 1.920110\n",
      "(Iteration 33 / 490) loss: 1.774214\n",
      "(Iteration 34 / 490) loss: 1.754605\n",
      "(Iteration 35 / 490) loss: 1.787106\n",
      "(Iteration 36 / 490) loss: 1.708336\n",
      "(Iteration 37 / 490) loss: 1.824147\n",
      "(Iteration 38 / 490) loss: 1.601345\n",
      "(Iteration 39 / 490) loss: 1.630245\n",
      "(Iteration 40 / 490) loss: 1.734971\n",
      "(Iteration 41 / 490) loss: 1.703734\n",
      "(Iteration 42 / 490) loss: 1.563948\n",
      "(Iteration 43 / 490) loss: 1.625060\n",
      "(Iteration 44 / 490) loss: 1.571623\n",
      "(Iteration 45 / 490) loss: 1.616550\n",
      "(Iteration 46 / 490) loss: 1.559544\n",
      "(Iteration 47 / 490) loss: 1.479870\n",
      "(Iteration 48 / 490) loss: 1.711460\n",
      "(Iteration 49 / 490) loss: 1.669606\n",
      "(Iteration 50 / 490) loss: 1.624371\n",
      "(Iteration 51 / 490) loss: 1.541933\n",
      "(Iteration 52 / 490) loss: 1.553665\n",
      "(Iteration 53 / 490) loss: 1.546523\n",
      "(Iteration 54 / 490) loss: 1.564817\n",
      "(Iteration 55 / 490) loss: 1.661760\n",
      "(Iteration 56 / 490) loss: 1.582042\n",
      "(Iteration 57 / 490) loss: 1.631158\n",
      "(Iteration 58 / 490) loss: 1.622046\n",
      "(Iteration 59 / 490) loss: 1.573650\n",
      "(Iteration 60 / 490) loss: 1.556816\n",
      "(Iteration 61 / 490) loss: 1.455417\n",
      "(Iteration 62 / 490) loss: 1.455280\n",
      "(Iteration 63 / 490) loss: 1.362863\n",
      "(Iteration 64 / 490) loss: 1.519286\n",
      "(Iteration 65 / 490) loss: 1.397870\n",
      "(Iteration 66 / 490) loss: 1.508168\n",
      "(Iteration 67 / 490) loss: 1.452331\n",
      "(Iteration 68 / 490) loss: 1.436994\n",
      "(Iteration 69 / 490) loss: 1.388707\n",
      "(Iteration 70 / 490) loss: 1.446086\n",
      "(Iteration 71 / 490) loss: 1.531266\n",
      "(Iteration 72 / 490) loss: 1.350548\n",
      "(Iteration 73 / 490) loss: 1.528835\n",
      "(Iteration 74 / 490) loss: 1.423426\n",
      "(Iteration 75 / 490) loss: 1.426968\n",
      "(Iteration 76 / 490) loss: 1.549216\n",
      "(Iteration 77 / 490) loss: 1.540855\n",
      "(Iteration 78 / 490) loss: 1.534212\n",
      "(Iteration 79 / 490) loss: 1.507521\n",
      "(Iteration 80 / 490) loss: 1.625511\n",
      "(Iteration 81 / 490) loss: 1.734085\n",
      "(Iteration 82 / 490) loss: 1.643222\n",
      "(Iteration 83 / 490) loss: 1.494868\n",
      "(Iteration 84 / 490) loss: 1.517391\n",
      "(Iteration 85 / 490) loss: 1.448477\n",
      "(Iteration 86 / 490) loss: 1.339369\n",
      "(Iteration 87 / 490) loss: 1.813973\n",
      "(Iteration 88 / 490) loss: 1.413489\n",
      "(Iteration 89 / 490) loss: 1.423666\n",
      "(Iteration 90 / 490) loss: 1.510072\n",
      "(Iteration 91 / 490) loss: 1.498428\n",
      "(Iteration 92 / 490) loss: 1.558521\n",
      "(Iteration 93 / 490) loss: 1.499652\n",
      "(Iteration 94 / 490) loss: 1.630630\n",
      "(Iteration 95 / 490) loss: 1.502495\n",
      "(Iteration 96 / 490) loss: 1.342494\n",
      "(Iteration 97 / 490) loss: 1.329704\n",
      "(Iteration 98 / 490) loss: 1.335494\n",
      "(Iteration 99 / 490) loss: 1.245668\n",
      "(Iteration 100 / 490) loss: 1.501248\n",
      "(Iteration 101 / 490) loss: 1.469281\n",
      "(Iteration 102 / 490) loss: 1.504701\n",
      "(Iteration 103 / 490) loss: 1.410656\n",
      "(Iteration 104 / 490) loss: 1.472590\n",
      "(Iteration 105 / 490) loss: 1.374005\n",
      "(Iteration 106 / 490) loss: 1.488776\n",
      "(Iteration 107 / 490) loss: 1.453482\n",
      "(Iteration 108 / 490) loss: 1.315704\n",
      "(Iteration 109 / 490) loss: 1.416663\n",
      "(Iteration 110 / 490) loss: 1.291410\n",
      "(Iteration 111 / 490) loss: 1.421395\n",
      "(Iteration 112 / 490) loss: 1.441303\n",
      "(Iteration 113 / 490) loss: 1.548337\n",
      "(Iteration 114 / 490) loss: 1.420507\n",
      "(Iteration 115 / 490) loss: 1.461793\n",
      "(Iteration 116 / 490) loss: 1.430712\n",
      "(Iteration 117 / 490) loss: 1.362901\n",
      "(Iteration 118 / 490) loss: 1.116315\n",
      "(Iteration 119 / 490) loss: 1.591380\n",
      "(Iteration 120 / 490) loss: 1.352535\n",
      "(Iteration 121 / 490) loss: 1.415962\n",
      "(Iteration 122 / 490) loss: 1.510480\n",
      "(Iteration 123 / 490) loss: 1.521213\n",
      "(Iteration 124 / 490) loss: 1.500055\n",
      "(Iteration 125 / 490) loss: 1.380335\n",
      "(Iteration 126 / 490) loss: 1.275413\n",
      "(Iteration 127 / 490) loss: 1.242838\n",
      "(Iteration 128 / 490) loss: 1.408513\n",
      "(Iteration 129 / 490) loss: 1.379379\n",
      "(Iteration 130 / 490) loss: 1.432027\n",
      "(Iteration 131 / 490) loss: 1.389097\n",
      "(Iteration 132 / 490) loss: 1.287287\n",
      "(Iteration 133 / 490) loss: 1.399034\n",
      "(Iteration 134 / 490) loss: 1.321609\n",
      "(Iteration 135 / 490) loss: 1.356942\n",
      "(Iteration 136 / 490) loss: 1.314662\n",
      "(Iteration 137 / 490) loss: 1.426808\n",
      "(Iteration 138 / 490) loss: 1.417625\n",
      "(Iteration 139 / 490) loss: 1.288730\n",
      "(Iteration 140 / 490) loss: 1.300897\n",
      "(Iteration 141 / 490) loss: 1.428950\n",
      "(Iteration 142 / 490) loss: 1.366203\n",
      "(Iteration 143 / 490) loss: 1.342609\n",
      "(Iteration 144 / 490) loss: 1.394677\n",
      "(Iteration 145 / 490) loss: 1.430298\n",
      "(Iteration 146 / 490) loss: 1.404081\n",
      "(Iteration 147 / 490) loss: 1.340256\n",
      "(Iteration 148 / 490) loss: 1.193359\n",
      "(Iteration 149 / 490) loss: 1.224840\n",
      "(Iteration 150 / 490) loss: 1.457964\n",
      "(Iteration 151 / 490) loss: 1.421520\n",
      "(Iteration 152 / 490) loss: 1.203536\n",
      "(Iteration 153 / 490) loss: 1.258473\n",
      "(Iteration 154 / 490) loss: 1.243706\n",
      "(Iteration 155 / 490) loss: 1.401342\n",
      "(Iteration 156 / 490) loss: 1.189841\n",
      "(Iteration 157 / 490) loss: 1.331043\n",
      "(Iteration 158 / 490) loss: 1.387942\n",
      "(Iteration 159 / 490) loss: 1.388664\n",
      "(Iteration 160 / 490) loss: 1.255160\n",
      "(Iteration 161 / 490) loss: 1.229665\n",
      "(Iteration 162 / 490) loss: 1.264237\n",
      "(Iteration 163 / 490) loss: 1.380586\n",
      "(Iteration 164 / 490) loss: 1.298371\n",
      "(Iteration 165 / 490) loss: 1.241009\n",
      "(Iteration 166 / 490) loss: 1.327663\n",
      "(Iteration 167 / 490) loss: 1.337484\n",
      "(Iteration 168 / 490) loss: 1.302907\n",
      "(Iteration 169 / 490) loss: 1.231443\n",
      "(Iteration 170 / 490) loss: 1.215407\n",
      "(Iteration 171 / 490) loss: 1.465839\n",
      "(Iteration 172 / 490) loss: 1.239122\n",
      "(Iteration 173 / 490) loss: 1.390618\n",
      "(Iteration 174 / 490) loss: 1.202011\n",
      "(Iteration 175 / 490) loss: 1.143867\n",
      "(Iteration 176 / 490) loss: 1.173835\n",
      "(Iteration 177 / 490) loss: 1.379769\n",
      "(Iteration 178 / 490) loss: 1.176810\n",
      "(Iteration 179 / 490) loss: 1.358017\n",
      "(Iteration 180 / 490) loss: 1.234791\n",
      "(Iteration 181 / 490) loss: 1.411576\n",
      "(Iteration 182 / 490) loss: 1.205878\n",
      "(Iteration 183 / 490) loss: 1.232974\n",
      "(Iteration 184 / 490) loss: 1.350179\n",
      "(Iteration 185 / 490) loss: 1.171462\n",
      "(Iteration 186 / 490) loss: 1.397376\n",
      "(Iteration 187 / 490) loss: 1.320067\n",
      "(Iteration 188 / 490) loss: 1.208866\n",
      "(Iteration 189 / 490) loss: 1.157807\n",
      "(Iteration 190 / 490) loss: 1.308461\n",
      "(Iteration 191 / 490) loss: 1.175226\n",
      "(Iteration 192 / 490) loss: 1.228405\n",
      "(Iteration 193 / 490) loss: 1.282390\n",
      "(Iteration 194 / 490) loss: 1.211563\n",
      "(Iteration 195 / 490) loss: 1.154806\n",
      "(Iteration 196 / 490) loss: 1.126135\n",
      "(Iteration 197 / 490) loss: 1.201161\n",
      "(Iteration 198 / 490) loss: 1.291895\n",
      "(Iteration 199 / 490) loss: 1.199262\n",
      "(Iteration 200 / 490) loss: 1.224205\n",
      "(Iteration 201 / 490) loss: 1.211654\n",
      "(Iteration 202 / 490) loss: 1.403841\n",
      "(Iteration 203 / 490) loss: 1.297765\n",
      "(Iteration 204 / 490) loss: 1.100145\n",
      "(Iteration 205 / 490) loss: 1.200488\n",
      "(Iteration 206 / 490) loss: 1.314814\n",
      "(Iteration 207 / 490) loss: 1.323187\n",
      "(Iteration 208 / 490) loss: 1.124139\n",
      "(Iteration 209 / 490) loss: 1.232043\n",
      "(Iteration 210 / 490) loss: 1.180868\n",
      "(Iteration 211 / 490) loss: 1.253376\n",
      "(Iteration 212 / 490) loss: 1.189344\n",
      "(Iteration 213 / 490) loss: 1.191245\n",
      "(Iteration 214 / 490) loss: 1.076675\n",
      "(Iteration 215 / 490) loss: 1.144718\n",
      "(Iteration 216 / 490) loss: 1.271757\n",
      "(Iteration 217 / 490) loss: 1.306035\n",
      "(Iteration 218 / 490) loss: 1.222047\n",
      "(Iteration 219 / 490) loss: 1.183928\n",
      "(Iteration 220 / 490) loss: 1.054933\n",
      "(Iteration 221 / 490) loss: 1.101040\n",
      "(Iteration 222 / 490) loss: 1.304183\n",
      "(Iteration 223 / 490) loss: 1.169736\n",
      "(Iteration 224 / 490) loss: 1.266414\n",
      "(Iteration 225 / 490) loss: 1.421639\n",
      "(Iteration 226 / 490) loss: 1.182887\n",
      "(Iteration 227 / 490) loss: 1.058243\n",
      "(Iteration 228 / 490) loss: 1.144026\n",
      "(Iteration 229 / 490) loss: 0.985753\n",
      "(Iteration 230 / 490) loss: 1.040200\n",
      "(Iteration 231 / 490) loss: 1.194988\n",
      "(Iteration 232 / 490) loss: 1.037374\n",
      "(Iteration 233 / 490) loss: 1.029663\n",
      "(Iteration 234 / 490) loss: 1.240261\n",
      "(Iteration 235 / 490) loss: 1.120763\n",
      "(Iteration 236 / 490) loss: 1.244743\n",
      "(Iteration 237 / 490) loss: 1.118118\n",
      "(Iteration 238 / 490) loss: 1.180420\n",
      "(Iteration 239 / 490) loss: 1.042560\n",
      "(Iteration 240 / 490) loss: 1.226867\n",
      "(Iteration 241 / 490) loss: 1.204200\n",
      "(Iteration 242 / 490) loss: 1.230075\n",
      "(Iteration 243 / 490) loss: 1.072451\n",
      "(Iteration 244 / 490) loss: 1.215954\n",
      "(Iteration 245 / 490) loss: 1.080835\n",
      "(Iteration 246 / 490) loss: 1.072021\n",
      "(Iteration 247 / 490) loss: 1.341777\n",
      "(Iteration 248 / 490) loss: 1.000145\n",
      "(Iteration 249 / 490) loss: 0.968483\n",
      "(Iteration 250 / 490) loss: 1.072735\n",
      "(Iteration 251 / 490) loss: 1.128640\n",
      "(Iteration 252 / 490) loss: 1.142389\n",
      "(Iteration 253 / 490) loss: 1.000583\n",
      "(Iteration 254 / 490) loss: 1.128797\n",
      "(Iteration 255 / 490) loss: 0.914008\n",
      "(Iteration 256 / 490) loss: 1.139542\n",
      "(Iteration 257 / 490) loss: 1.137704\n",
      "(Iteration 258 / 490) loss: 1.113153\n",
      "(Iteration 259 / 490) loss: 1.030190\n",
      "(Iteration 260 / 490) loss: 1.181656\n",
      "(Iteration 261 / 490) loss: 1.003456\n",
      "(Iteration 262 / 490) loss: 1.159914\n",
      "(Iteration 263 / 490) loss: 1.187593\n",
      "(Iteration 264 / 490) loss: 1.037672\n",
      "(Iteration 265 / 490) loss: 1.129425\n",
      "(Iteration 266 / 490) loss: 1.125691\n",
      "(Iteration 267 / 490) loss: 1.065668\n",
      "(Iteration 268 / 490) loss: 0.976091\n",
      "(Iteration 269 / 490) loss: 1.093805\n",
      "(Iteration 270 / 490) loss: 1.349515\n",
      "(Iteration 271 / 490) loss: 1.028192\n",
      "(Iteration 272 / 490) loss: 1.076557\n",
      "(Iteration 273 / 490) loss: 1.155249\n",
      "(Iteration 274 / 490) loss: 1.096586\n",
      "(Iteration 275 / 490) loss: 1.218893\n",
      "(Iteration 276 / 490) loss: 1.134635\n",
      "(Iteration 277 / 490) loss: 1.130677\n",
      "(Iteration 278 / 490) loss: 0.955831\n",
      "(Iteration 279 / 490) loss: 1.082208\n",
      "(Iteration 280 / 490) loss: 0.841883\n",
      "(Iteration 281 / 490) loss: 1.126821\n",
      "(Iteration 282 / 490) loss: 1.111677\n",
      "(Iteration 283 / 490) loss: 1.009199\n",
      "(Iteration 284 / 490) loss: 1.004007\n",
      "(Iteration 285 / 490) loss: 1.309274\n",
      "(Iteration 286 / 490) loss: 1.179129\n",
      "(Iteration 287 / 490) loss: 1.307517\n",
      "(Iteration 288 / 490) loss: 1.122198\n",
      "(Iteration 289 / 490) loss: 1.068414\n",
      "(Iteration 290 / 490) loss: 1.151899\n",
      "(Iteration 291 / 490) loss: 1.088699\n",
      "(Iteration 292 / 490) loss: 1.093639\n",
      "(Iteration 293 / 490) loss: 1.008130\n",
      "(Iteration 294 / 490) loss: 1.109742\n",
      "(Iteration 295 / 490) loss: 0.808728\n",
      "(Iteration 296 / 490) loss: 1.015851\n",
      "(Iteration 297 / 490) loss: 1.402190\n",
      "(Iteration 298 / 490) loss: 1.037402\n",
      "(Iteration 299 / 490) loss: 0.954301\n",
      "(Iteration 300 / 490) loss: 1.069301\n",
      "(Iteration 301 / 490) loss: 1.046656\n",
      "(Iteration 302 / 490) loss: 1.051951\n",
      "(Iteration 303 / 490) loss: 1.105148\n",
      "(Iteration 304 / 490) loss: 1.101783\n",
      "(Iteration 305 / 490) loss: 1.133231\n",
      "(Iteration 306 / 490) loss: 1.151719\n",
      "(Iteration 307 / 490) loss: 1.190498\n",
      "(Iteration 308 / 490) loss: 0.950256\n",
      "(Iteration 309 / 490) loss: 1.033115\n",
      "(Iteration 310 / 490) loss: 1.147362\n",
      "(Iteration 311 / 490) loss: 0.998203\n",
      "(Iteration 312 / 490) loss: 1.136513\n",
      "(Iteration 313 / 490) loss: 1.044210\n",
      "(Iteration 314 / 490) loss: 1.117441\n",
      "(Iteration 315 / 490) loss: 1.068312\n",
      "(Iteration 316 / 490) loss: 1.192371\n",
      "(Iteration 317 / 490) loss: 1.100688\n",
      "(Iteration 318 / 490) loss: 1.127001\n",
      "(Iteration 319 / 490) loss: 1.139160\n",
      "(Iteration 320 / 490) loss: 0.957918\n",
      "(Iteration 321 / 490) loss: 0.974095\n",
      "(Iteration 322 / 490) loss: 0.946230\n",
      "(Iteration 323 / 490) loss: 1.178034\n",
      "(Iteration 324 / 490) loss: 1.135440\n",
      "(Iteration 325 / 490) loss: 1.380641\n",
      "(Iteration 326 / 490) loss: 1.238701\n",
      "(Iteration 327 / 490) loss: 1.273918\n",
      "(Iteration 328 / 490) loss: 1.159501\n",
      "(Iteration 329 / 490) loss: 1.107992\n",
      "(Iteration 330 / 490) loss: 0.933056\n",
      "(Iteration 331 / 490) loss: 0.993943\n",
      "(Iteration 332 / 490) loss: 0.964459\n",
      "(Iteration 333 / 490) loss: 1.118900\n",
      "(Iteration 334 / 490) loss: 0.857311\n",
      "(Iteration 335 / 490) loss: 1.023434\n",
      "(Iteration 336 / 490) loss: 1.060122\n",
      "(Iteration 337 / 490) loss: 0.952464\n",
      "(Iteration 338 / 490) loss: 1.075465\n",
      "(Iteration 339 / 490) loss: 0.962155\n",
      "(Iteration 340 / 490) loss: 0.882718\n",
      "(Iteration 341 / 490) loss: 1.106520\n",
      "(Iteration 342 / 490) loss: 1.032614\n",
      "(Iteration 343 / 490) loss: 1.006316\n",
      "(Iteration 344 / 490) loss: 1.293261\n",
      "(Iteration 345 / 490) loss: 1.133954\n",
      "(Iteration 346 / 490) loss: 0.941449\n",
      "(Iteration 347 / 490) loss: 0.932245\n",
      "(Iteration 348 / 490) loss: 0.969928\n",
      "(Iteration 349 / 490) loss: 1.084395\n",
      "(Iteration 350 / 490) loss: 1.183392\n",
      "(Iteration 351 / 490) loss: 0.982472\n",
      "(Iteration 352 / 490) loss: 0.920959\n",
      "(Iteration 353 / 490) loss: 1.079311\n",
      "(Iteration 354 / 490) loss: 0.965696\n",
      "(Iteration 355 / 490) loss: 0.794623\n",
      "(Iteration 356 / 490) loss: 1.032436\n",
      "(Iteration 357 / 490) loss: 1.067995\n",
      "(Iteration 358 / 490) loss: 0.960412\n",
      "(Iteration 359 / 490) loss: 1.018695\n",
      "(Iteration 360 / 490) loss: 1.114409\n",
      "(Iteration 361 / 490) loss: 0.940158\n",
      "(Iteration 362 / 490) loss: 0.881523\n",
      "(Iteration 363 / 490) loss: 0.886873\n",
      "(Iteration 364 / 490) loss: 1.052093\n",
      "(Iteration 365 / 490) loss: 1.003257\n",
      "(Iteration 366 / 490) loss: 0.948189\n",
      "(Iteration 367 / 490) loss: 1.063578\n",
      "(Iteration 368 / 490) loss: 1.056742\n",
      "(Iteration 369 / 490) loss: 1.050499\n",
      "(Iteration 370 / 490) loss: 0.837325\n",
      "(Iteration 371 / 490) loss: 0.918692\n",
      "(Iteration 372 / 490) loss: 1.027358\n",
      "(Iteration 373 / 490) loss: 1.189513\n",
      "(Iteration 374 / 490) loss: 1.040921\n",
      "(Iteration 375 / 490) loss: 0.913610\n",
      "(Iteration 376 / 490) loss: 1.020113\n",
      "(Iteration 377 / 490) loss: 0.965542\n",
      "(Iteration 378 / 490) loss: 1.089877\n",
      "(Iteration 379 / 490) loss: 0.979706\n",
      "(Iteration 380 / 490) loss: 1.207570\n",
      "(Iteration 381 / 490) loss: 1.182701\n",
      "(Iteration 382 / 490) loss: 1.186238\n",
      "(Iteration 383 / 490) loss: 0.793791\n",
      "(Iteration 384 / 490) loss: 1.029067\n",
      "(Iteration 385 / 490) loss: 0.805898\n",
      "(Iteration 386 / 490) loss: 0.845079\n",
      "(Iteration 387 / 490) loss: 0.860274\n",
      "(Iteration 388 / 490) loss: 1.105978\n",
      "(Iteration 389 / 490) loss: 0.850942\n",
      "(Iteration 390 / 490) loss: 0.957771\n",
      "(Iteration 391 / 490) loss: 0.737134\n",
      "(Iteration 392 / 490) loss: 1.020838\n",
      "(Iteration 393 / 490) loss: 1.044487\n",
      "(Iteration 394 / 490) loss: 0.999701\n",
      "(Iteration 395 / 490) loss: 1.204617\n",
      "(Iteration 396 / 490) loss: 1.002647\n",
      "(Iteration 397 / 490) loss: 0.809914\n",
      "(Iteration 398 / 490) loss: 1.019369\n",
      "(Iteration 399 / 490) loss: 1.008183\n",
      "(Iteration 400 / 490) loss: 1.019518\n",
      "(Iteration 401 / 490) loss: 1.087025\n",
      "(Iteration 402 / 490) loss: 0.903641\n",
      "(Iteration 403 / 490) loss: 0.899529\n",
      "(Iteration 404 / 490) loss: 0.985504\n",
      "(Iteration 405 / 490) loss: 0.855009\n",
      "(Iteration 406 / 490) loss: 1.073804\n",
      "(Iteration 407 / 490) loss: 0.723031\n",
      "(Iteration 408 / 490) loss: 0.896756\n",
      "(Iteration 409 / 490) loss: 0.996235\n",
      "(Iteration 410 / 490) loss: 0.939061\n",
      "(Iteration 411 / 490) loss: 0.948544\n",
      "(Iteration 412 / 490) loss: 0.917879\n",
      "(Iteration 413 / 490) loss: 0.942603\n",
      "(Iteration 414 / 490) loss: 0.903240\n",
      "(Iteration 415 / 490) loss: 1.095670\n",
      "(Iteration 416 / 490) loss: 0.815237\n",
      "(Iteration 417 / 490) loss: 0.924848\n",
      "(Iteration 418 / 490) loss: 0.900090\n",
      "(Iteration 419 / 490) loss: 0.989104\n",
      "(Iteration 420 / 490) loss: 1.048782\n",
      "(Iteration 421 / 490) loss: 1.101755\n",
      "(Iteration 422 / 490) loss: 0.783475\n",
      "(Iteration 423 / 490) loss: 1.047447\n",
      "(Iteration 424 / 490) loss: 1.026549\n",
      "(Iteration 425 / 490) loss: 0.901366\n",
      "(Iteration 426 / 490) loss: 0.991468\n",
      "(Iteration 427 / 490) loss: 1.060972\n",
      "(Iteration 428 / 490) loss: 1.001615\n",
      "(Iteration 429 / 490) loss: 1.197954\n",
      "(Iteration 430 / 490) loss: 1.152292\n",
      "(Iteration 431 / 490) loss: 0.826339\n",
      "(Iteration 432 / 490) loss: 0.859805\n",
      "(Iteration 433 / 490) loss: 0.747964\n",
      "(Iteration 434 / 490) loss: 0.942301\n",
      "(Iteration 435 / 490) loss: 1.068959\n",
      "(Iteration 436 / 490) loss: 0.796777\n",
      "(Iteration 437 / 490) loss: 0.849713\n",
      "(Iteration 438 / 490) loss: 0.942562\n",
      "(Iteration 439 / 490) loss: 0.942150\n",
      "(Iteration 440 / 490) loss: 0.891561\n",
      "(Iteration 441 / 490) loss: 0.990107\n",
      "(Iteration 442 / 490) loss: 0.845924\n",
      "(Iteration 443 / 490) loss: 0.946170\n",
      "(Iteration 444 / 490) loss: 0.868148\n",
      "(Iteration 445 / 490) loss: 1.005208\n",
      "(Iteration 446 / 490) loss: 0.836265\n",
      "(Iteration 447 / 490) loss: 0.873711\n",
      "(Iteration 448 / 490) loss: 0.889946\n",
      "(Iteration 449 / 490) loss: 0.778783\n",
      "(Iteration 450 / 490) loss: 0.832935\n",
      "(Iteration 451 / 490) loss: 0.818042\n",
      "(Iteration 452 / 490) loss: 1.181361\n",
      "(Iteration 453 / 490) loss: 0.883725\n",
      "(Iteration 454 / 490) loss: 0.872388\n",
      "(Iteration 455 / 490) loss: 1.072916\n",
      "(Iteration 456 / 490) loss: 0.848609\n",
      "(Iteration 457 / 490) loss: 0.878533\n",
      "(Iteration 458 / 490) loss: 1.021499\n",
      "(Iteration 459 / 490) loss: 0.911630\n",
      "(Iteration 460 / 490) loss: 0.974948\n",
      "(Iteration 461 / 490) loss: 0.635431\n",
      "(Iteration 462 / 490) loss: 0.924543\n",
      "(Iteration 463 / 490) loss: 0.826927\n",
      "(Iteration 464 / 490) loss: 1.129283\n",
      "(Iteration 465 / 490) loss: 1.066751\n",
      "(Iteration 466 / 490) loss: 0.770487\n",
      "(Iteration 467 / 490) loss: 0.866426\n",
      "(Iteration 468 / 490) loss: 1.006467\n",
      "(Iteration 469 / 490) loss: 0.896424\n",
      "(Iteration 470 / 490) loss: 1.010279\n",
      "(Iteration 471 / 490) loss: 0.790992\n",
      "(Iteration 472 / 490) loss: 0.771109\n",
      "(Iteration 473 / 490) loss: 0.886185\n",
      "(Iteration 474 / 490) loss: 1.089430\n",
      "(Iteration 475 / 490) loss: 0.992162\n",
      "(Iteration 476 / 490) loss: 0.886280\n",
      "(Iteration 477 / 490) loss: 1.009499\n",
      "(Iteration 478 / 490) loss: 0.994891\n",
      "(Iteration 479 / 490) loss: 1.076227\n",
      "(Iteration 480 / 490) loss: 0.814904\n",
      "(Iteration 481 / 490) loss: 0.741352\n",
      "(Iteration 482 / 490) loss: 0.993733\n",
      "(Iteration 483 / 490) loss: 0.927917\n",
      "(Iteration 484 / 490) loss: 0.957082\n",
      "(Iteration 485 / 490) loss: 0.774466\n",
      "(Iteration 486 / 490) loss: 0.924416\n",
      "(Iteration 487 / 490) loss: 0.830986\n",
      "(Iteration 488 / 490) loss: 0.844965\n",
      "(Iteration 489 / 490) loss: 1.040209\n",
      "(Iteration 490 / 490) loss: 0.941487\n",
      "(Epoch 1 / 1) train acc: 0.686000; val_acc: 0.671000\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "from cs231n.classifiers.convnet import *\n",
    "from cs231n.solver import Solver\n",
    "\n",
    "hl = [{'name': 'conv'}, {'name': 'conv'}, {'name': 'pool'},\n",
    "        {'name': 'conv'}, {'name': 'conv'}, {'name': 'pool'},\n",
    "        {'name': 'affine'}]\n",
    "\n",
    "# lr_decay of 0.75 works better;\n",
    "# 1. is used for debugging the \n",
    "# equivalent keras model in\n",
    "# keras_cifar10.ipynb\n",
    "lr_decays = [1.]\n",
    "\n",
    "solvers = []\n",
    "for decay in lr_decays:\n",
    "    model = ConvNet(weight_scale=0.001, affine_dim=500, hidden_layers=hl)\n",
    "\n",
    "    solver = Solver(model, data,\n",
    "                    num_epochs=1, batch_size=100,\n",
    "                    update_rule='adam',\n",
    "                    lr_decay=decay,\n",
    "                    optim_config={\n",
    "                      'learning_rate': 1e-3,\n",
    "                    },\n",
    "                    verbose=True, print_every=1)\n",
    "    solver.train()\n",
    "    solvers.append(solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
